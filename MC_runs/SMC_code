#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat May 17 12:20:26 2025

@author: Aloush97
"""

import numpy as np
from multiprocessing import Pool
from functools import partial
from typing import Optional, Tuple
import pickle
import pandas as pd
import os
from scipy.sparse.linalg import spsolve
from scipy.sparse import csr_matrix
from scipy.stats import multivariate_normal



# ----------------------
#  functions
# ----------------------



def FVM_steady_state_ligand_con(neigbourhood_list, mesh_properties, param_A, param_B, cell_densities):
    """
    solve for the growth factor/ligand concentration at the FVM centres
    
    Inputs:
    
    cell_densities: an N_CV by n array, where N_CV is number of CVs, n is number of
    cell types if interest.
    
    neighbourhood list: a list of arrays describing the neigbours for each CV
    mesh properties: Area of CV, L_c: distance between centroids, L_e: length of edge
    parameter_vector: D: diffusion coefficient, lambda_g: decay constant, 
    rho: production rates (of size n), 
    K: association constant (scaled with receptor number) (of size n)
    
    Output: concentration of ligand at the CV (assuming steady-state)
    """
    N_CV = cell_densities.shape[0]
    n= cell_densities.shape[1]
    Area, L_c, L_e = mesh_properties
    D, lambda_g = param_A
    rho = param_B[:n]
    K = param_B[n]
    rec_number = param_B[-n:]
    
    A = np.zeros((N_CV, N_CV))
    b = np.zeros((N_CV))
    for i in range(N_CV):
        neighbours = neigbourhood_list[i]
        lig_prod = cell_densities[i,:]@rho
        lig_absor = cell_densities[i,:]@(rec_number*K)
        A[i,i] = D*L_e/L_c * len(neighbours) + Area*(lambda_g+lig_absor)
        A[i, neighbours] = -D*L_e/L_c
        b[i] = Area*lig_prod
    A = csr_matrix(A*1e11)
    b= b*1e23    
    g = spsolve(A,b)
    return g #in pmol/m^2
    
def calc_interaction_strength(cell_densities, g_conc, param):
    
    N_CV = cell_densities.shape[0]
    n = cell_densities.shape[1]
    
    rho = param[:n]
    K = param[n]
    rec_number = param[-n:]  #size n
        

        
    num_1 = cell_densities*rho   #Size CV by n
    denom_1 = np.sum(num_1, axis=1)  #size CV
    prod_term = num_1/denom_1[:,None]  #size CV by n
        
    absor_array = K*rec_number[:,None] * g_conc[None,:] #size n by CV: outer product
    interaction_strength_array = 	np.matmul(absor_array, prod_term)
    #where first dimensions (rows) correspond the receiving cells and second dimension
    #(columns) refers to producing cells
    return 1/N_CV*interaction_strength_array
        
def calc_correlation(I1, I2, I3, I_data):
    """
    Inputs:
    I1, I2, and I3 are for g1 (TGFB1), g2 (TGFB2) and g3 (TGFB3): rows are receiving cells
    #and columns are g producing cells
    
    interaction_strength_data: n by n by #ligands (3) here; each slice corresponds to a ligand
    
    
    #both data from model and scRNA-seq are arranged such that rows are receiving cells and
    #columns are ligand producing cells
    
    Output: float number signifiying correlation coefficient
    
    """
    
    I_model = np.concatenate((I1.flatten(), I2.flatten(), I3.flatten()))
    I_data = I_data.flatten()

    return np.corrcoef(I_model, I_data)[0, 1]





# ----------------------
# Mesh Configuration
# ----------------------
L_c = 84.5e-6 #from smallest distance 
hotspot_area = np.sqrt(3)/2*L_c**2
L_e = L_c/np.sqrt(3)

mesh_properties = [hotspot_area,L_c,L_e]
with open('dataframe.pkl', 'rb') as file:
    neighbours_info = pickle.load(file)
neighbours=neighbours_info.iloc[:, 1].tolist() 

q05_cell_numbers = pd.read_csv('q05_cell_abundances.csv')       
q05_cell_numbers.columns.values[0] = 'Nucleotide_ID'
are_identical = neighbours_info['ID'].equals(q05_cell_numbers['Nucleotide_ID'])
print("Are the columns identical?", are_identical)
 

cells_of_interest = ('FB-I', 'FB-II', 'FB-III', 'Mono-Mac', 'VE')

# Create new tuple with prefixes
prefix = 'q05cell_abundance_w_sf_'
column_names = tuple(f"{prefix}{cell}" for cell in cells_of_interest)

# Suppose df is your original DataFrame
# Extract those columns
cell_numbers_of_interest = q05_cell_numbers[list(column_names)].copy()

# Rename columns to the original short names
cell_numbers_of_interest.columns = cells_of_interest
cell_densities = cell_numbers_of_interest.to_numpy()/hotspot_area

###Prepare the interaction strength data as 3D array, rows are receiving cells, 
#columns are ligand producing cells, and 3rd dimension is for the different ligands
file_path = 'donor 3 day 30 filtered tgfb data.ods'

# Read each sheet into a dictionary of DataFrames
sheets = ['TGFB1-TGBFR2', 'TGFB3-TGFBR2', 'TGFB2-TGFBR3']
dfs = {sheet: pd.read_excel(file_path, sheet_name=sheet, engine='odf') for sheet in sheets}

# Now, we can extract the 5x5 tables from each sheet into a 3D NumPy array
# We slice each DataFrame to remove the first row and first column

interaction_data = np.array([dfs[sheet].iloc[:, 1:].values for sheet in sheets])





#-----------------------------------
#Monte Carlo Filtering configuration
#-----------------------------------

MC = int(1e6)
# Sample from prior
rec_const = 6.0214*10**23 #assuming 10um tissue sample thickness

# Sample from prior
D = 10**(np.random.uniform(np.log10(2.13)-11, np.log10(1.3)-10, MC)) #m^2/s
lambda_g = np.random.uniform(4.1e-6, 1.332e-5, MC) #1/s
K3_2 = 10**(np.random.uniform(np.log10(6.9)+2, np.log10(8.3)+4, MC))
K1_2 = 10**(np.random.uniform(np.log10(5.1)+2, np.log10(3.1)+4, MC))
K2_3 = 10**(np.random.uniform(np.log10(5)+2, np.log10(2)+4, MC)) #in m^3/(s.mol)
rho_fib_1 = np.random.uniform(1.556,3.472,MC)*1e-22 #mol/(s.cell)
rho_fib_2 = np.random.uniform(1.556,3.472,MC)*1e-22
rho_fib_3 = np.random.uniform(1.556,3.472,MC)*1e-22
rho_mac = 0.5* np.random.uniform(1.556,3.472,MC)*1e-22
rho_endo = 0.2* np.random.uniform(1.556,3.472,MC)*1e-22
rec_number_fib_1 = np.random.uniform(8350,14850,MC)/rec_const
rec_number_fib_2 = np.random.uniform(8350,14850,MC)/rec_const
rec_number_fib_3 = np.random.uniform(8350,14850,MC)/rec_const
rec_number_ECs = np.random.uniform(6000,13200,MC)/rec_const
rec_number_mac =  (10**(np.random.uniform(np.log10(3.8)+2, np.log10(8)+3, MC)))/rec_const

realizations = np.column_stack([D, lambda_g, rho_fib_1, rho_fib_2, rho_fib_3, \
                                rho_mac, rho_endo,K1_2, K2_3, K3_2,\
                                rec_number_fib_1, rec_number_fib_2, rec_number_fib_3,\
                                rec_number_mac, rec_number_ECs])

epsilon = 0.1

# Segment indices #need not to be continuous! segment A for D, lambda, segment B for the rhos and Ks
seg_A = np.array([0,1])
seg_1B = np.concatenate([np.arange(2, 8) , np.arange(10,15)])
seg_2B = np.concatenate([np.arange(2, 7) , np.array([8]), np.arange(10,15)])
seg_3B = np.concatenate([np.arange(2, 7) , np.arange(9,15)])
#seg 1_B, 2_B, and 3_B refer to ligands (TGFB) 1, 2, and 3.

# ----------------------
# Segment computation
# ----------------------
def compute_segment(r_i: np.ndarray, segA: np.ndarray, segB: np.ndarray, neighbours, mesh_prop, cell_dens) -> float:
    g =  FVM_steady_state_ligand_con(neighbours, mesh_prop, r_i[segA], r_i[segB], cell_dens)     
    y = calc_interaction_strength(cell_dens,g,r_i[segB])
    return y

# ----------------------
# Per-realization evaluation
# ----------------------
def evaluate_realization(r_i, I_emp, neighbours, mesh_prop, cell_dens, epsilon) -> Optional[float]]:
    

    # Directly compute the three segments without inner pool
    I1 = compute_segment(r_i, seg_A, seg_1B, neighbours, mesh_prop, cell_dens)
    I2 = compute_segment(r_i, seg_A, seg_2B, neighbours, mesh_prop, cell_dens)
    I3 = compute_segment(r_i, seg_A, seg_3B, neighbours, mesh_prop, cell_dens)

    score = calc_correlation(I1, I2, I3, I_emp)

    if score >= epsilon:
        return score
    return None






    


def prior_dis_calc(MC_realizations):
    """
    

    Parameters
    ----------
    MC_realizations : an MC by d array of accepted realizationd

    Returns
    -------
    mean: d vector of the means
    std: d vector of the standard deviations
    dist: a multivariate distribution of the standardized variables

    """
    return None
  
def prior_dis_pdf_calc():
    
    return None

def prior_dis_sample():
    return None


def mvn_pdf_vectorized(x, mean, cov):
    """
    x: (N, d) points where to evaluate PDF
    mean: (d,) mean vector
    cov: (d, d) covariance matrix
    Returns: (N,) array of pdf values
    """
    d = mean.size
    L = np.linalg.cholesky(cov)  # (d, d)
    
    diff = x - mean  # (N, d)
    
    # Solve L y = diff.T for y: y = L^{-1} (x - mean)
    # diff.T shape: (d, N), solve for all points at once
    y = np.linalg.solve(L, diff.T)  # (d, N)
    
    # Compute squared Mahalanobis distance for each point
    maha_sq = np.sum(y**2, axis=0)  # (N,)
    
    # Compute normalization constant
    norm_const = 1.0 / (np.power(2 * np.pi, d / 2) * np.prod(np.diag(L)))
    
    return norm_const * np.exp(-0.5 * maha_sq)



prior_dis, mean_prior, std_prior = prior_dis_calc() #multivariate distribution; normalized 
#calculate from accepted MC_samples fitting
d = 34  #dimension of parameter space
epsilon_0 = 0.4
T = int(21)
N = int(5e3) #number of particles
population_scores = np.zeros([T,N])
epsilon_vector = np.zeros(T)
epsilon_vector[0] = epsilon_0
#different processing for T=0
theta_t_minus = np.zeros([N,d])
theta_new = np.zeros([N,d])
acceptance_threshold = 1e-4
cov_scaling_constant=1
particle_realization = np.zeros((T,N,d))

#for T=0 
for i in range(N): #can parallelize
    theta_star = prior_dis_sample(size=1) #sample fro prior
    score = None
    while not score:
        unnormalized_sample = theta_star*std_prior+mean_prior
        score = evaluate_realization(unormalized_sample,interaction_data,\
                                  neighbours, mesh_properties, cell_densities, epsilon_0)
    theta_t_minus[i,:] = theta_star
    population_scores[0,i] = score
particle_realization[0,:,:] = theta_t_minus
weights_old= 1/N *np.ones(N)
kernel_cov_matrix =  np.cov(theta_t_minus,rowvar=False)*cov_scaling_constant+\
                    + 1e-10 * np.eye(d)


for t in range(1,T):
    epsilon_t = np.percentile(population_scores[t-1,:], 75)
    epsilon_vector[t] = epsilon_t
    weights_new = np.zeros(N)  
    for i in range(N): #parallelize
        score = None
        while (score is None):
            sample_number = np.random.choice(list(range(N)), p=weights_old)[0] #make sure it is
            #     #with replacement
            theta_star = np.random.multivariate_normal(theta_t_minus[sample_number,:],\
                                         kernel_cov_matrix )#apply pertrubration kernel
            #check if pertrubed realization is inside prior pdf
            #evaluate prior pdf of theta_star and check realization is in prior
            unnormalized_sample = std_prior*theta_star+mean_prior
            prior_pdf_i = prior_dis_pdf_calc(unnormalized_sample)
            if prior_pdf_i >=acceptance_threshold:
                score = evaluate_realization(unormalized_sample,interaction_data,\
                            neighbours, mesh_properties, cell_densities, epsilon_t)
        theta_new[i,:] = theta_star
        population_scores[t,i] = score
        #calculate the weight for particle i
        transition_vector = mvn_pdf_vectorized(theta_t_minus,theta_star, kernel_cov_matrix)
            #backward kernel evaluation from theta_star to previous particle realizations
        weights_new[i] = prior_pdf_i/np.dot(weights_old, transition_vector)
    theta_t_minus=theta_new
    particle_realization[t,:,:] = theta_t_minus
    weights_old = weights_new/np.sum(weights_new)       
    kernel_cov_matrix = np.cov(theta_t_minus,rowvar=False)*cov_scaling_constant +\
         1e-10 * np.eye(d)
    


    
    
    # scores = np.zeros(N)
    # unormalized_samples = std_t_minus*theta_stars+mean_t_minus
    
    # accepted_scores = np.array([])
    # accepted_realizations = np.empty((0, d))
    # L=N
    # while len(accepted_scores<N)
    #     sample_numbers = np.random.choice(list(range(N)), size=L, p=weights_old) #make sure it is
    #     #with replacement
    #     theta_stars = theta_t_minus[sample_numbers,:] + \
    #     np.random.multivariate_normal(0, kernel_cov_matrix, size=L)
    #     for i in range(L): #create parallel loop to calculate scores
    #         prior_pdf_i = prior_dis_pdf_calc(unnormalized_samples[i]) 
    #         scores[i] = evaluate_realization(unormalized_samples[i],interaction_data,\
    #                     neighbours, mesh_properties, cell_densities, espsilon_t)
    #     accepted_scores_indices = np.where(scores>=epsilon_t & 
    #                                        prior_pdf_i>acceptance_threshold)[0]
    #     accepted_scores = np.concatenate((accepted_scores, scores[accepted_scores_indices]))
    #     accepted_realization = np.vstack([accepted_realizations, new_data])
    # repeat_scores = np.delete()
    
    # while repeat_scores: 
    #     L = len(scores[scores<epsilon_t])
    #     sample_numbers = np.random.choice(list(range(N)), size=L, p=weights_old)
    #     theta_replace = theta_t_minus[sample_numbers,:] + \
    #     p.random.multivariate_normal(0, kernel_cov_matrix, size=L)
    #     unormalized_samples = std_t_minus*theta_replacemean_t_minus
    #     scores_replace
    #     for l in range(L): #also paralleize
    #         prior_pdf_l = prior_dis_pdf_calc(unnormalized_samples[l])
    #         if prior_pdf_l>acceptance_threshold: 
    #             scores[i] = evaluate_realization(unormalized_samples[i],interaction_data,\
    #                 neighbours, mesh_properties, cell_densities, espsilon_t)
    #     repeat_scores = np.where(scores<epsilon_t)[0]

